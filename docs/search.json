[
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "Analyzing the sentiment towards transportation in Reddit posts on r/SanAntonio\n\n# Initializing tidytext and dplyr libraries\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.2\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\nlibrary(stopwords)\n\n# Using reddit data from another class.\nredditdata &lt;- read.csv(\"redditdata.csv\", comment.char=\"#\")\n\n# Rearranging data so that there is one work per line and it only contains posts related to transportation\ntidy_reddit &lt;- redditdata %&gt;% \n  filter(Transportation == 1) %&gt;% \n  unnest_tokens(word, Post)\n\n# Remove stop words\ntidy_reddit &lt;- tidy_reddit %&gt;% anti_join(get_stopwords())\n\nJoining with `by = join_by(word)`\n\n# Join with sentiment data and count the number of positive and negative words\ntransportationsentiment &lt;- tidy_reddit %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = \"word\", relationship = \"many-to-many\") %&gt;%\n  count(ID, sentiment)\n\n# Get the sentiment from each post by subtracting negative count from positive count\ntransportationsentiment &lt;- transportationsentiment %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n\n# Get the average of the sentiment score from the posts\nmean(transportationsentiment$sentiment)\n\n[1] 0.4683544\n\n\nBased on this analysis, Reddit posts in r/SanAntonio tend to have neutral sentiment towards transportation."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "# Loads the dslabs and tidyverse packages\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.3.2\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# view gapminder help file\nhelp(gapminder)\n\n# view data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n\n\n# summary statistics for the data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n\n\n# get the object type of the data\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\n\n# seperate out Africa data\nafricadata &lt;- gapminder %&gt;% filter(continent == 'Africa')\n\n# view data structure\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# summary statistics for the data\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\n\n# keep only infant_mortality and life_expectancy columns and save as im_le\nim_le &lt;- africadata %&gt;% select(infant_mortality,life_expectancy)\n\n# view data structure\nstr(im_le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\n# summary statistics for the data\nsummary(im_le)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\n\n# keep only population and life_expectancy columns and save as p_le\np_le &lt;- africadata %&gt;% select(population,life_expectancy)\n\n# view data structure\nstr(p_le)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n# summary statistics for the data\nsummary(p_le)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n\n\n# create a scatter plot of life_expenctancy as a function of infant_mortality\nplot(im_le$infant_mortality, im_le$life_expectancy)\n\n\n\n\n\n# create a scatter plot of life_expenctancy as a function of the natural log of population\nplot(log(p_le$population), p_le$life_expectancy)\n\n\n\n\nStreaks are due to countries have multiple data points in the dataset (multiple years are reported for one country). Therefore, we are seeing the correlations over different time periods.\n\n# filter africadata to only include rows with no data for infant_mortality and then get the count of year values.\nafricadata %&gt;% filter(is.na(infant_mortality)) %&gt;% count(year)\n\n   year  n\n1  1960 10\n2  1961 17\n3  1962 16\n4  1963 16\n5  1964 15\n6  1965 14\n7  1966 13\n8  1967 11\n9  1968 11\n10 1969  7\n11 1970  5\n12 1971  6\n13 1972  6\n14 1973  6\n15 1974  5\n16 1975  5\n17 1976  3\n18 1977  3\n19 1978  2\n20 1979  2\n21 1980  1\n22 1981  1\n23 2016 51\n\n\n\n# seperate out data from 2000\nafricadata2000 &lt;- africadata %&gt;% filter(year == 2000)\n\n# view data structure\nstr(africadata2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# summary statistics for the data\nsummary(africadata2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\n\n# create a scatter plot of life_expenctancy as a function of infant_mortality\nplot(africadata2000$infant_mortality, africadata2000$life_expectancy)\n\n\n\n\n\n# create a scatter plot of life_expenctancy as a function of the natural log of population\nplot(log(africadata2000$population), africadata2000$life_expectancy)\n\n\n\n\n\n# linear regression of life_expectancy by infant_mortality\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data=africadata2000)\n\n# linear regression of life_expectancy by population\nfit2 &lt;- lm(life_expectancy ~ population, data=africadata2000)\n\n# summary of the infant_mortality model\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\n\n# summary of the population model\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nSince the p-value of the infant_mortality model is less than .05, there is a statistically significant linear relationship between infant mortality and life expectancy This is not true for the population model, so there does not exist a strong linear relationship between population and life expectancy.\n\nTHIS SECTION CONTRIBUTED TO BY ANTONIO FLORES\nChoosing to analyze the ‘us_contagious_diseases’ dataset from gapminder.\n\n#Calling the data from gapminder package\ndata('us_contagious_diseases')\n#Transferring to data frame with simpler name\ndisease = us_contagious_diseases\n\n\n# Checking structure of the data\nstr(disease)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\n#Looks like all the data types are usable\n\n\n#Checking the data quartiles and some samples\nsummary(disease)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n\n\n#Creating a variable to show how many cases per 100,000 people in the state have contracted the disease\ndisease$Per100K = (disease$count /(disease$population / 100))\n\n\n#Adding package for better visualizations\nlibrary(ggplot2)\n\n\n#Bar chart showing which diseases have been most severe in our dataset\ndisease |&gt;\n  ggplot()+\n    geom_bar(aes(x=disease, y=count), stat=\"identity\")\n\n\n\n\n\n#Plotting the total diseases per year\n#We can see what we would expect, less cases as medicince modernizes\nplot(disease$year, disease$count)\n\n\n\n\n\n#Similar chart to the previous one, but now we're seeing comparing diseases to each other for reference\nggplot(data=disease, aes(x=year, y=count, group=disease))+\n  geom_line(aes(color=disease))\n\n\n\n\n\n#Bar plot showing which states have had the most cases of diseases overall\ndisease |&gt;\n  ggplot()+\n    geom_bar(aes(x=reorder(state, count), y=count), stat=\"identity\")+\n    coord_flip()\n\n\n\n#Using the reorder command so that states with the most cases are at the top\n#Using the coord_flip command so the state names are on the left side\n\n\n#Generating the same plot but now we're using the cases per 100,000 variable we created earliar.\ndisease |&gt;\n  ggplot()+\n    geom_bar(aes(x=reorder(state, Per100K), y=Per100K), stat=\"identity\")+\n    coord_flip()\n\nWarning: Removed 214 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n#We see a different set of states when we account for population size\n\n\n#Plotting to determine which weeks have most cases reported\nplot(disease$weeks_reporting, disease$count)\n\n\n\n\n\n#First Model: Predictor is weeks reported and outcome is number of cases\n\nfit1 = lm(count ~ weeks_reporting, data=disease)\nsummary(fit1)\n\n\nCall:\nlm(formula = count ~ weeks_reporting, data = disease)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -2350  -1924  -1292    239 129992 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -700.075    107.580  -6.507 7.87e-11 ***\nweeks_reporting   58.664      2.606  22.515  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5794 on 16063 degrees of freedom\nMultiple R-squared:  0.03059,   Adjusted R-squared:  0.03053 \nF-statistic: 506.9 on 1 and 16063 DF,  p-value: &lt; 2.2e-16\n\n\nWe see a very significant effect as indicated by the low p-value, but a fairly low R^2 value which means that our model doesn’t account for much of the variance in the data\n\n#Second Model: Predictor is cases per 100K and outcome is number of cases\n\nfit2 = lm(count ~ Per100K, data=disease)\nsummary(fit2)\n\n\nCall:\nlm(formula = count ~ Per100K, data = disease)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-63566   -267   -239   -179  98594 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   239.05      37.34   6.402 1.58e-10 ***\nPer100K     24944.84     227.85 109.481  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4468 on 15849 degrees of freedom\n  (214 observations deleted due to missingness)\nMultiple R-squared:  0.4306,    Adjusted R-squared:  0.4306 \nF-statistic: 1.199e+04 on 1 and 15849 DF,  p-value: &lt; 2.2e-16\n\n\nWe see there is a significant effect caused the number of cases per 100K and a greater R^2 value then our previous model. ."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "# Call ggplot2 and tidymodels libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6      ✔ rsample      1.2.1 \n✔ dials        1.2.1      ✔ tibble       3.2.1 \n✔ dplyr        1.1.4      ✔ tidyr        1.3.1 \n✔ infer        1.0.7      ✔ tune         1.2.1 \n✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n✔ purrr        1.0.2      ✔ yardstick    1.3.1 \n✔ recipes      1.0.10     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.2\n\n\nWarning: package 'parsnip' was built under R version 4.3.2\n\n\nWarning: package 'recipes' was built under R version 4.3.2\n\n\nWarning: package 'rsample' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'tune' was built under R version 4.3.2\n\n\nWarning: package 'workflows' was built under R version 4.3.2\n\n\nWarning: package 'workflowsets' was built under R version 4.3.2\n\n\nWarning: package 'yardstick' was built under R version 4.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(timetk)\nlibrary(smooth)\n\nWarning: package 'smooth' was built under R version 4.3.3\n\n\nLoading required package: greybox\n\n\nWarning: package 'greybox' was built under R version 4.3.3\n\n\nRegistered S3 method overwritten by 'greybox':\n  method     from\n  print.pcor lava\nPackage \"greybox\", v2.0.1 loaded.\n\n\nAttaching package: 'greybox'\n\nThe following object is masked from 'package:lubridate':\n\n    hm\n\nThe following object is masked from 'package:yardstick':\n\n    accuracy\n\nThe following object is masked from 'package:tidyr':\n\n    spread\n\nThis is package \"smooth\", v4.0.2\n\n\nAttaching package: 'smooth'\n\nThe following object is masked from 'package:yardstick':\n\n    accuracy\n\nThe following object is masked from 'package:parsnip':\n\n    pls\n\n\n\n# Get ratings data from GitHub\nratings &lt;- readr::read_csv(\"https://raw.githubusercontent.com/kkakey/American_Idol/main/metadata/ratings.csv\")\n\nRows: 593 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): episode, airdate, 18_49_rating_share, timeslot_et, dvr_18_49, dvr_...\ndbl  (4): season, show_number, viewers_in_millions, nightlyrank\nlgl  (1): ref\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Graph of the number of viewrs by week and season\nggplot(ratings) + geom_line(aes(x=show_number, y=viewers_in_millions)) + facet_grid(. ~ season)\n\n\n\n\n\n# Graph of the week rank by week and season\nggplot(ratings) + geom_line(aes(x=show_number, y=weekrank)) + facet_grid(. ~ season)\n\n\n\n\n\n# Graphy of viewers by week and season\nggplot(ratings) + geom_line(aes(x=show_number, y=viewers_in_millions, group=season, color=season))\n\n\n\n\n\n# Dropping NA viewship rows\nratings &lt;- ratings %&gt;% drop_na(viewers_in_millions)\n\n# Tuning models on season and show number to predict viewership\nset.seed(192)\nmm &lt;- mars(mode = \"regression\", num_terms = 5) %&gt;%\n    fit(viewers_in_millions ~ season + show_number, data = ratings)\n\nWarning: package 'earth' was built under R version 4.3.2\n\n\nWarning: package 'plotmo' was built under R version 4.3.2\n\n\n\nAttaching package: 'plotrix'\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\nsm &lt;- svm_rbf(mode = \"regression\", rbf_sigma = 0.2) %&gt;%\n    fit(viewers_in_millions ~ season + show_number, data = ratings)\n\nfm &lt;- rand_forest(mode = \"regression\", trees = 2000) %&gt;%\n  fit(viewers_in_millions ~ season + show_number, data = ratings)\n\n\n# Creating test data\nr2 &lt;- ratings %&gt;% select(season, show_number)\n\n# Getting predictions\nmp &lt;- predict(mm, r2)\nsp &lt;- predict(sm, r2)\nfp &lt;- predict(fm, r2)\n\n\n# Calculating RMSE\nsqrt(mean((ratings$viewers_in_millions - mp$.pred)^2))\n\n[1] 2.8697\n\nsqrt(mean((ratings$viewers_in_millions - sp$.pred)^2))\n\n[1] 2.651979\n\nsqrt(mean((ratings$viewers_in_millions - fp$.pred)^2))\n\n[1] 1.604036\n\n\nThe questions I attempted to answer was how the season and show impact ratings. Based on exploratory analysis, it looked like viewship had peaked and was decreasing in future seasons. Based on the models created, the random forest performed the best with the lowest RMSE."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Benjamin Farias\n\n\n\nTrip to Colorado, including a visit to the CU Boulder campus in January 2023\n\n\n\nEducation\nI graduated from The University of Texas at Austin in May 2023 where I earned my BBA in Management Information Systems. I additionally completed a certificate in Applied Statistical Modeling. My certificate prompted me to continue my education in the UTSA MSDA program.\n\n\nExperience\nI am currently a Technology Risk Staff at EY, where I have gained experience in ITGC and ITAC evaluation for banks and insurance companies. My work in the technology risk practice has exposed me to a large amount of technology systems critical to the operations of financial institutions. Prior to joining EY, I served as an AmeriCorps member at the San Antonio Food Bank. I served on the data specialist’s team and performed data processing and cleaning for food distributions in San Antonio and the surrounding area. I additionally used data from these distributions to create Tableau visualizations demonstrating the need for food in the area. As a student I worked in UT’s Institutional Reporting, Research, Information and Surveys department.\n\n\nPersonal\nIn my free time, I enjoy traveling, attending Spurs games, and reading. Lately, I have enjoyed reading John Green’s The Anthropocene Reviewed.\n\n\nResource\nThe Map the Meal Gap by Feeding America provides interesting information about the state of food insecurity across America and some of the factors that impact food insecurity. View the data at https://map.feedingamerica.org."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Benjamin Farias’ Data Analysis Portfolio",
    "section": "",
    "text": "UTSA MSDA Practicum II\n\nSummer 2024\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around.\nHave fun!\n\nFeel free to change this text any way you want 😁!"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "# Initializing tidyverse\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# CDC data of the percent of the population over 60 covered by RSV vaccine. Estimate is the percent vaccinated in the week for the specified state.\n\n# Read data\ndata &lt;- read.csv(\"rsv.csv\", comment.char=\"#\")\n\n# Fiter to only include state level data and filter out NAs\ndata &lt;- data %&gt;% filter(Geographic.Level == 'State') %&gt;% filter(!(is.na(Estimate)))\n\n# Histogram of the estimates. Looks to be fairly normal\nhist(data$Estimate)\n\n\n\n# Mean of estimate\nmean(data$Estimate)\n\n[1] 23.28765\n\n# Standard deviation of estimate\nsd(data$Estimate)\n\n[1] 12.02931\n\n# Percentage of state\ndata %&gt;% group_by(Geographic.Name) %&gt;% summarise(percent = 100 * n() / 7168)\n\n# A tibble: 54 × 2\n   Geographic.Name      percent\n   &lt;chr&gt;                  &lt;dbl&gt;\n 1 Alabama                 2.19\n 2 Alaska                  1.80\n 3 Arizona                 2.13\n 4 Arkansas                1.86\n 5 California              2.13\n 6 Colorado                2.08\n 7 Connecticut             1.02\n 8 Delaware                2.02\n 9 District of Columbia    2.19\n10 Florida                 2.13\n# ℹ 44 more rows\n\n# Percentage of weeks\ndata %&gt;% group_by(Week_ending) %&gt;% summarise(percent = 100 * n() / 7168)\n\n# A tibble: 33 × 2\n   Week_ending            percent\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 01/06/2024 12:00:00 AM    3.57\n 2 01/13/2024 12:00:00 AM    3.57\n 3 01/20/2024 12:00:00 AM    3.29\n 4 01/27/2024 12:00:00 AM    3.29\n 5 02/03/2024 12:00:00 AM    3.52\n 6 02/10/2024 12:00:00 AM    3.07\n 7 02/17/2024 12:00:00 AM    3.07\n 8 02/24/2024 12:00:00 AM    3.12\n 9 03/02/2024 12:00:00 AM    3.57\n10 03/09/2024 12:00:00 AM    3.18\n# ℹ 23 more rows"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#creating-synthetic-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#creating-synthetic-data",
    "title": "CDC Data Exercise",
    "section": "Creating Synthetic Data",
    "text": "Creating Synthetic Data\n\nnew_seed = 2024 # Setting our seed at a random value\nsynthetic_data = syn(subset, \n                     seed = new_seed) #This will give us our synthetic dataset\n\nVariables CI_Half_width_90pct, CI_Half_width_95pct are collinear. Variables later in 'visit.sequence'\nare derived from CI_Half_width_90pct.\n\n\nSynthesis\n-----------\n Geographic.Name Estimate Unweighted.Sample.Size CI_Half_width_90pct CI_Half_width_95pct\n\n\n\n#cleaning the synthetic dataset\nsynth_data_clean = sdc(synthetic_data, subset, \n                       label = \"FAKE DATA\",\n                       rm.replicated.uniques = TRUE) \n\nno. of replicated uniques: 50\n\n\nThis is important! Sometimes, when we create “fake” synthetic data, we happen to recreate an actual observation in the original data set. So, in the name of privacy, we remove those observations. In this exercise, our synthetic data created 50 “fake” entries that actually had a match with real observations in the original data set.\n\nnewdata = synth_data_clean$syn\n\n# Histogram of the estimates. Looks to be fairly normal\nhist(newdata$Estimate)\n\n\n\n# Mean of estimate\nmean(newdata$Estimate)\n\n[1] 23.11207\n\n# Standard deviation of estimate\nsd(newdata$Estimate)\n\n[1] 11.94228\n\n# Percentage of state\nnewdata %&gt;% group_by(Geographic.Name) %&gt;% summarise(percent = 100 * n() / 7168)\n\n# A tibble: 54 × 2\n   Geographic.Name      percent\n   &lt;fct&gt;                  &lt;dbl&gt;\n 1 Alabama                 2.12\n 2 Alaska                  1.93\n 3 Arizona                 1.95\n 4 Arkansas                2.30\n 5 California              2.08\n 6 Colorado                2.02\n 7 Connecticut             1.13\n 8 Delaware                2.04\n 9 District of Columbia    2.08\n10 Florida                 2.25\n# ℹ 44 more rows"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#conclusions",
    "href": "cdcdata-exercise/cdcdata-exercise.html#conclusions",
    "title": "CDC Data Exercise",
    "section": "Conclusions:",
    "text": "Conclusions:\nFirstly, I had to subset the columns into just one categorical because the Synthpop tool was taking an egregious amount of time to load (which makes sense).\nSecond, the data looks to have many of the same distributions for the specified variable (Estimate). The Histograms visually seem to be very similar, both means are at 23, and the standard deviations are fairly close (12.0, 11.9)."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Recreating the plot from https://fivethirtyeight.com/features/where-fivethirtyeight-and-espns-2022-23-nba-forecasts-agree-and-disagree/\n\n# Calling the required libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(ggthemes)\n\nWarning: package 'ggthemes' was built under R version 4.3.2\n\n# Input the data which was manually compiled from the page.\ndata &lt;- read.csv(\"NBAProjections.csv\", comment.char=\"#\")\n\n# Create the plot with BPI on x axis and RAPTOR on y axis, color points orange with black outline. Add title. Add regression line. Add vertical line at x=0. Make it in the Fivethirthyeight theme.\nggplot(data, aes(x = BPI, y = RAPTOR)) +\n  geom_point(color='orange',size=3, border='black') +\n  geom_point(shape = 1,size = 3,colour = \"black\") +\n  ggtitle(\"Who does RAPTOR like more than BPI? Uh ... the Raptors.\") +\n  geom_smooth(method = \"lm\", se = FALSE, color='black', size=.5) +\n  geom_vline(xintercept = 0, color='black', size=.5)\n\nWarning in geom_point(color = \"orange\", size = 3, border = \"black\"): Ignoring\nunknown parameters: `border`\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n  theme_fivethirtyeight()\n\nList of 136\n $ line                            :List of 6\n  ..$ colour       : chr \"black\"\n  ..$ linewidth    : num 0.545\n  ..$ linetype     : num 1\n  ..$ lineend      : chr \"butt\"\n  ..$ arrow        : logi FALSE\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_line\" \"element\"\n $ rect                            :List of 5\n  ..$ fill         : Named chr \"#F0F0F0\"\n  .. ..- attr(*, \"names\")= chr \"Light Gray\"\n  ..$ colour       : logi NA\n  ..$ linewidth    : num 0.545\n  ..$ linetype     : num 0\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ text                            :List of 11\n  ..$ family       : chr \"sans\"\n  ..$ face         : chr \"plain\"\n  ..$ colour       : Named chr \"#3C3C3C\"\n  .. ..- attr(*, \"names\")= chr \"Dark Gray\"\n  ..$ size         : num 12\n  ..$ hjust        : num 0.5\n  ..$ vjust        : num 0.5\n  ..$ angle        : num 0\n  ..$ lineheight   : num 0.9\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : logi FALSE\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ title                           : NULL\n $ aspect.ratio                    : NULL\n $ axis.title                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.title.x                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 3points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.top                :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 3points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.x.bottom             : NULL\n $ axis.title.y                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num 90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 3points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.title.y.left               : NULL\n $ axis.title.y.right              :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : num -90\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 3points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text                       :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 1\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 2.4points 0points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.top                 :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : NULL\n  ..$ vjust        : num 0\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 2.4points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.x.bottom              : NULL\n $ axis.text.y                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.4points 0points 0points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.y.left                : NULL\n $ axis.text.y.right               :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 0points 0points 2.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.text.theta                 : NULL\n $ axis.text.r                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0.5\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : 'margin' num [1:4] 0points 2.4points 0points 2.4points\n  .. ..- attr(*, \"unit\")= int 8\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ axis.ticks                      : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.ticks.x                    : NULL\n $ axis.ticks.x.top                : NULL\n $ axis.ticks.x.bottom             : NULL\n $ axis.ticks.y                    : NULL\n $ axis.ticks.y.left               : NULL\n $ axis.ticks.y.right              : NULL\n $ axis.ticks.theta                : NULL\n $ axis.ticks.r                    : NULL\n $ axis.minor.ticks.x.top          : NULL\n $ axis.minor.ticks.x.bottom       : NULL\n $ axis.minor.ticks.y.left         : NULL\n $ axis.minor.ticks.y.right        : NULL\n $ axis.minor.ticks.theta          : NULL\n $ axis.minor.ticks.r              : NULL\n $ axis.ticks.length               : 'simpleUnit' num 3points\n  ..- attr(*, \"unit\")= int 8\n $ axis.ticks.length.x             : NULL\n $ axis.ticks.length.x.top         : NULL\n $ axis.ticks.length.x.bottom      : NULL\n $ axis.ticks.length.y             : NULL\n $ axis.ticks.length.y.left        : NULL\n $ axis.ticks.length.y.right       : NULL\n $ axis.ticks.length.theta         : NULL\n $ axis.ticks.length.r             : NULL\n $ axis.minor.ticks.length         : 'rel' num 0.75\n $ axis.minor.ticks.length.x       : NULL\n $ axis.minor.ticks.length.x.top   : NULL\n $ axis.minor.ticks.length.x.bottom: NULL\n $ axis.minor.ticks.length.y       : NULL\n $ axis.minor.ticks.length.y.left  : NULL\n $ axis.minor.ticks.length.y.right : NULL\n $ axis.minor.ticks.length.theta   : NULL\n $ axis.minor.ticks.length.r       : NULL\n $ axis.line                       : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ axis.line.x                     : NULL\n $ axis.line.x.top                 : NULL\n $ axis.line.x.bottom              : NULL\n $ axis.line.y                     : NULL\n $ axis.line.y.left                : NULL\n $ axis.line.y.right               : NULL\n $ axis.line.theta                 : NULL\n $ axis.line.r                     : NULL\n $ legend.background               :List of 5\n  ..$ fill         : NULL\n  ..$ colour       : logi NA\n  ..$ linewidth    : NULL\n  ..$ linetype     : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_rect\" \"element\"\n $ legend.margin                   : 'margin' num [1:4] 6points 6points 6points 6points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing                  : 'simpleUnit' num 12points\n  ..- attr(*, \"unit\")= int 8\n $ legend.spacing.x                : NULL\n $ legend.spacing.y                : NULL\n $ legend.key                      : NULL\n $ legend.key.size                 : 'simpleUnit' num 1.2lines\n  ..- attr(*, \"unit\")= int 3\n $ legend.key.height               : NULL\n $ legend.key.width                : NULL\n $ legend.key.spacing              : 'simpleUnit' num 6points\n  ..- attr(*, \"unit\")= int 8\n $ legend.key.spacing.x            : NULL\n $ legend.key.spacing.y            : NULL\n $ legend.frame                    : NULL\n $ legend.ticks                    : NULL\n $ legend.ticks.length             : 'rel' num 0.2\n $ legend.axis.line                : NULL\n $ legend.text                     :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : 'rel' num 0.8\n  ..$ hjust        : NULL\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.text.position            : NULL\n $ legend.title                    :List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 0\n  ..$ vjust        : NULL\n  ..$ angle        : NULL\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi TRUE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n $ legend.title.position           : NULL\n $ legend.position                 : chr \"bottom\"\n $ legend.position.inside          : NULL\n $ legend.direction                : chr \"horizontal\"\n $ legend.byrow                    : NULL\n $ legend.justification            : chr \"center\"\n $ legend.justification.top        : NULL\n $ legend.justification.bottom     : NULL\n $ legend.justification.left       : NULL\n $ legend.justification.right      : NULL\n $ legend.justification.inside     : NULL\n $ legend.location                 : NULL\n $ legend.box                      : chr \"vertical\"\n $ legend.box.just                 : NULL\n $ legend.box.margin               : 'margin' num [1:4] 0cm 0cm 0cm 0cm\n  ..- attr(*, \"unit\")= int 1\n $ legend.box.background           : list()\n  ..- attr(*, \"class\")= chr [1:2] \"element_blank\" \"element\"\n $ legend.box.spacing              : 'simpleUnit' num 12points\n  ..- attr(*, \"unit\")= int 8\n  [list output truncated]\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi TRUE\n - attr(*, \"validate\")= logi TRUE\n\n\nUsed ChatGPT to generate this code. The prompt provided was “Write r code to create a table in the style of the website fivethirthyeight from a data frame called data with 3 columns called”Team”, “BPI”, and “RAPTOR”“. The code was modified to make it work with the data that was read in earlier.\n\n# Call libraries\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.3.3\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Create the table\ntable &lt;- data %&gt;%\n  gt() %&gt;%\n  # Apply FiveThirtyEight-like styling\n  tab_header(\n    title = md(\"**Team Performance Metrics**\"),\n    subtitle = md(\"Source: Fivethrithyeight and ESPN\")\n  ) %&gt;%\n  cols_label(\n    Team = \"Team\",\n    BPI = \"BPI\",\n    RAPTOR = \"RAPTOR\"\n  ) %&gt;%\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_column_labels(everything())\n  ) %&gt;%\n  tab_options(\n    table.font.names = \"Arial\",\n    table.font.size = px(12),\n    heading.title.font.size = px(18),\n    heading.subtitle.font.size = px(14),\n    heading.align = \"left\",\n    table.border.top.style = \"none\",\n    table.border.bottom.color = \"gray\",\n    column_labels.border.bottom.color = \"gray\",\n    column_labels.border.bottom.width = px(2),\n    table_body.border.bottom.color = \"gray\",\n    table_body.border.bottom.width = px(1),\n    table_body.border.top.style = \"none\",\n    table.border.left.style = \"none\",\n    table.border.right.style = \"none\"\n  )\n\n# Print the table\ntable\n\n\n\n\n\n  \n    \n      Team Performance Metrics\n\n    \n    \n      Source: Fivethrithyeight and ESPN\n\n    \n    \n      Team\n      BPI\n      RAPTOR\n    \n  \n  \n    Oklahoma City Thunder\n-7.27\n1349\n    Detroit Pistons\n-6.74\n1348\n    Houston Rockets\n-6.37\n1322\n    Orlando Magic\n-6.34\n1358\n    San Antonio Spurs\n-5.30\n1446\n    Portland Trail Blazers\n-3.59\n1525\n    Indiana Pacers\n-3.35\n1492\n    Sacremento Kings\n-1.89\n1470\n    New York Knicks\n-1.53\n1553\n    Chicago Bulls\n-1.15\n1497\n    Washington Wizards\n-0.28\n1467\n    Los Angeles Lakers\n0.10\n1470\n    Utah Jazz\n0.15\n1510\n    Los Angeles Clippers\n0.30\n1602\n    Charlotte Hornets\n0.71\n1506\n    Toronto Raptors\n0.65\n1640\n    New Orleans Pelicans\n1.59\n1556\n    Minnesota Timberwolves\n1.83\n1605\n    Miami Heat\n1.78\n1647\n    Denver Nuggets\n2.29\n1650\n    Cleveland Caviliers\n2.43\n1580\n    Brooklyn Nets\n2.54\n1616\n    Memphis Grizzlies\n2.66\n1634\n    Golden State Warriors\n3.15\n1622\n    Milwaukee Bucks\n3.20\n1611\n    Dallas Mavericks\n3.22\n1638\n    Phoenix Suns\n3.68\n1622\n    Atlanta Hawks\n3.86\n1618\n    Philidelphia 76ers\n6.06\n1642\n    Boston Celtics\n6.58\n1693"
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Predicting the Results of College Football Games",
    "section": "",
    "text": "Warning: package 'knitr' was built under R version 4.3.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Predicting the Results of College Football Games",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nThe college football season consists of 128 D1 FBS teams that play 12 games a season. The teams are split into multiple conferences. Teams will play 8-9 games against teams in the same conference with the remainder of games being played against non-conference teams. Additionally with the expansion of the College Football Playoffs, teams will play a more diverse set of teams in the post-season (Dinich, 2024). This makes it useful to be able to compare teams despite not playing equal schedules."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Predicting the Results of College Football Games",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nThe data is from https://collegefootballdata.com and is pulled and imported using the CFBFastR library. The website contains game data by team and player as well as play-by-play data and advanced statistics for all D1 football games."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Predicting the Results of College Football Games",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nHow a team’s historical data can be used to predict how teams will perform in the future; however, the data will be skewed by conferences as the teams at the top of each conference will appear to perform similarly despite not playing equal opponents."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Predicting the Results of College Football Games",
    "section": "3.1 Data import and cleaning",
    "text": "3.1 Data import and cleaning\nData was pulled in for all weeks of the season with the CFBFastR package. Some values were pulled in as the wrong datatype, so key data was converted to the correct data type. The weekly box scores were then aggregated to include averages at each week in the season. To aid in logistic regression, the averages for before the week of the game were combined and subtracted resulting in differences. In these differences, the postive indicates the home team having the advantage, and the negative indicating the away team had the avantage. These differences were then used in the regression models. For full code, refer to the statistical-analysis.R"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Predicting the Results of College Football Games",
    "section": "3.2 Statistical analysis",
    "text": "3.2 Statistical analysis\nMultiple models were created to attempt to predict the outcome of games. A linear model was used to predict the score difference and then win was given to the team with the higher predicted score. As yards and sacks had the most linear relationship with points, they were the only variables included in the model.\nA logistic model was also created to predict whether the home team would win the game. This model included more variables that impacted the winner of the game.\nAs the goal is to predict the outcome of the game, models were evalulated based on getting the outcome correct, with more emphasis placed on predicting the correct outcome later in the season. For the full analysis, refer to the statistical-analysis.R file."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Predicting the Results of College Football Games",
    "section": "3.3 Full analysis",
    "text": "3.3 Full analysis\nBoth models were used to predict whether the home team would win the game. Table 1 shows a summary of the linear model’s classification accuracy. The model correctly predicted the winner in 68% of games in the 2023 season. The logistic model performed similarly, as shown in Table 2, with a 68% accuraracy rate, however the logistic model seemed to predict the home team to win more often, indicating that it gave a higher importance to home field advantage.\n\n\n\n\nTable 1: Linear model fit table.\n\n\n\nFALSE\nTRUE\n\n\n\n\nFALSE\n108\n63\n\n\nTRUE\n99\n239\n\n\n\n\n\n\n\n\n\n\nTable 2: Logisitic model fit table.\n\n\n\nFALSE\nTRUE\n\n\n\n\nFALSE\n100\n55\n\n\nTRUE\n107\n247"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Predicting the Results of College Football Games",
    "section": "4.1 Summary and Interpretation",
    "text": "4.1 Summary and Interpretation\nBoth models were able to accurately predict the outcome of a game in 68% of games in the 2023 season. This is better than a coin flip and outperforms analysts who are correct in about 60% of their predictions (Newman, 2020)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Predicting the Results of College Football Games",
    "section": "4.2 Strengths and Limitations",
    "text": "4.2 Strengths and Limitations\nBoth models make use of basic box score statistics in predicting the outcome of the game. This makes the models more understandable as box score statistics are commonly referenced. However these statistics are limited in what can be measured in the game. Advanced statistics can account for some of this gap, but are more difficult to understand.\nThe models could also be improved using some form of weighting to account for differences in schedule strength. One method to do this is with an ELO ranking that would indicate the comparative strength of teams based on historical results (Radjewski, 2021)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Predicting the Results of College Football Games",
    "section": "4.3 Conclusions",
    "text": "4.3 Conclusions\nThis project has been an excellent learning experience in exploring and preparing data for modeling, creating and evaluating models, and communicating results. The models I created performed better than I expected with the basic statistics they used for fitting; however, there is room for improvement to address some of the limitations previously mentioned."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data.\nThe code produces a few tables and figures\nIn order to pull the data, please ensure you have cfbfastR installed. Lines 7-10 install directly from GitHub as install.package may not be up to date for all versions of R.\nThe early analysis includes game results and box scores. Currently, I am identifying variables that may have a relationship to points. I also plan on working on a model that will take the difference of season averages to fit a logiistic model."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/bfarias/Documents/School/Summer 2024/PracticumII/BenjaminFarias-P2-Portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/bfarias/Documents/School/Summer 2024/PracticumII/BenjaminFarias-P2-Portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Benjamin Farias Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  }
]